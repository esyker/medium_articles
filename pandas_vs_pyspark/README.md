# Pandas vs PySpark Comparison Project

## Overview
This project compares the performance, functionality, and usability of **Pandas** and **PySpark** for data processing tasks. It highlights the strengths and weaknesses of each library when working with datasets of varying sizes.

## Motivation & Key Questions
- **Pandas** is optimal for small to medium-sized datasets due to its simplicity and robust functionality.
- **PySpark** is designed for distributed computing and excels with large-scale data.
- **Key Questions:**
  - How does Pandas perform compared to PySpark for small datasets?
  - At what data scale does PySpark start outperforming Pandas?
  - What are the trade-offs between ease of use and scalability?

## Features
- Benchmarking performance for operations such as filtering, grouping, and aggregations.
- Comparison of syntax simplicity and ease of use.
- Analysis of scalability differences.

## Requirements & Installation

### Dependencies
The project requires the Python libraries, as specified in `requirements.txt`.
